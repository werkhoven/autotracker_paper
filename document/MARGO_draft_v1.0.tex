\documentclass[10pt]{article}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage[caption=false]{subfig}
\usepackage{soul}
\usepackage[top=0.75in, bottom=0.75in, left=0.5in, right=0.5in]{geometry}
\usepackage[margin=1.5cm]{caption}
\usepackage{epsfig,amsmath}
\DeclarePairedDelimiter\abs{${\lvert}$}{${\rvert}$}%
\usepackage{titlesec,color}
\usepackage{kpfonts}
\usepackage{empheq}
\usepackage{palatino}
\usepackage{graphicx,wrapfig}
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}
\usepackage{array}
\usepackage{gensymb}
\usepackage{soul}
\usepackage{grffile}
\usepackage{listings}
\usepackage{color}
\usepackage{tcolorbox}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{helvet}
\usepackage{multicol}
\usepackage[super,sort&compress]{natbib}
\setlength\columnsep{0.3in}
\setlength{\parindent}{1cm}
\setlength{\parskip}{1mm}
\tcbuselibrary{listings,skins}
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=1000
\hypersetup{citecolor=black}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}
	
\renewcommand*\rmdefault{phv}
\fontfamily{phv}\selectfont

\newcommand{\avg}[1]{\left<{#1}\right>}
\newcommand{\hence}{\hspace{1cm}\Longrightarrow\hspace{1cm}}
\renewcommand{\ni}{\noindent}
\newcommand{\din}{\indent \indent}
\newcommand{\mni}{\medskip \noindent}
\newcommand{\bni}{\bigskip \noindent}
\newcommand{\sni}{\smallskip \noindent}
\newcommand{\pr}{{\rm Prob}}
\newcommand{\mon}{\begin{displaymath}}
\newcommand{\moff}{\end{displaymath}}
\newcommand{\sumi}[1]{\sum_{{#1}=-\infty}^{\infty}}
\renewcommand{\b}[1]{\mbox{\boldmath ${#1}$}}
\newcommand{\sumy}{\sum_{\b{y}}}
\newcommand{\sumz}{\sum_{\b{z}}}
\newcommand{\pd}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\od}[2]{\frac{d {#1}}{d {#2}}}
\newcommand{\odat}[3]{\left. \frac{d {#1}}{d {#2}} \right|_{#3}}
\newcommand{\inti}{\int_{-\infty}^{\infty}}
\newcommand{\eon}{\begin{equation}}
\newcommand{\eoff}{\end{equation}}
\newcommand{\eaon}{\begin{eqnarray}}
\newcommand{\eaoff}{\end{eqnarray}}
\newcommand{\e}[1]{\times 10^{#1}}
\newcommand{\chem}[2]{{}^{#2} \mathrm{#1}}
\renewcommand{\sb}{s}
\newcommand{\s}{s}
\newcommand{\zetaexp}{\left( \zeta e^{q \s t} \right)}
\newcommand{\taunuc}{\tau_{nuc}}
\newcommand{\eq}[1]{Eq. (\ref{#1})}\
\newcommand{\ev}[1]{\langle #1 \rangle}
\newcommand*\mean[1]{${\bar{#1}}$}
\newcolumntype{L}{>{\centering\arraybackslash}m{5cm}}
\bibliographystyle{unsrtnat}


\vspace*{\stretch{1.0}}
\begin{center}
	\LARGE\textbf{MARGO (Massively Automated Real time GUI for Object-tracking), a platform for high-throughput ethology}\\
	\large\textbf{}
\end{center}
\vspace*{\stretch{2.0}}
\rightline{\textbf{Zach Werkhoven}, \textbf{Chuan Qin}, \textbf{Benjamin de Bivort}}
\rightline{\small Dept. of Organismic and Evolutionary Biology, Harvard University, Cambridge MA, 02138}

\section*{Abstract}

Automated animal tracking offers the potential to probe the mechanisms underlying behavior more deeply than comparable manual approaches. Fast tracking in real time allows convenient tracking of very large numbers of animals or conducting experiments that require closed-loop stimulus control for multiple animals in parallel. We developed MARGO, a real time animal tracking suite for conducting custom behavioral experiments. We demonstrated that MARGO can rapidly and accurately track large numbers of animals in parallel. We show that MARGO's robustness to changes in image background and tracking speed make it feasible to tracking large cohorts of animals over very long timescales. Additionally, we incorporated control of peripheral hardware, and implemented a convenient software architecture for defining new experimental routines. These features enable closed-loop delivery of stimuli to many individuals simultaneously. We highlight MARGO's ability to tightly coordinate tracking and hardware control in parallel with two custom behavioral assays (measuring phototaxis and optomotor response). There are currently several open source software packages for animal tracking. MARGO’s strength is being a flexible platform with the potential to accelerate a wide-variety of behavioral experiments and bring sophisticated stimulus control to the realm of large-scale experimentation and screening.

\vspace{1cm}
\begin{multicols}{2}
\section*{Introduction}

Automated animal tracking methods have become commonplace in the study of behavior. They enable large sample sizes, high statistical power, and more rapid inference of the various mechanisms giving rise to behavior. Existing animal trackers vary in computational complexity and specialization to particular imaging configurations. In part, tracker diversity can be attributed to the diversity of desired behavioral measurements. Trackers can assist in a wide range of experimental tasks such as monitoring activity, measuring response to stimuli\cite{Fry_TrackFly_2008,Donelson_High_2012}, and recording postural changes over time\cite{Mathis_DeepLabCut_2018,Pereira_Fast_2018}. Some trackers are designed to track and maintain identities of multiple individuals occupying the same arena \cite{Prez-Escudero_idTracker_2014,Eyjolfsdottir_Detecting_2014,Rodriguez_ToxId_2017} while others measure the collective activity of populations without maintaining identities or rely on physical segregation of animals to ensure trajectories never collide \cite{Ramot_The_2008,Swierczek_High_2011,Itskovits_A_2017}. But few of these trackers are designed as platforms for high throughput, hardware control, and flexible experimental reconfiguration. 

Improvements in machine learning and template matching approaches to object localization and classification have made it possible to efficiently train models that accurately track and classify a variety of animal species and visually distinguish identities of individuals across time \cite{Eyjolfsdottir_Detecting_2014,Prez-Escudero_idTracker_2014}. Tracking individual identity in groups requires resolving identities through collisions where bodies are overlapping. Tracking applications idTracker and FlyTracker train classifiers to assign identities to individuals in each frame and can also be used to extract postural information such head and limb position. When trained well, these trackers can maintain distinct identities over extended periods with minimal human intervention. Other applications, such as Ctrax and Tracktor\cite{Branson_High_2009,Rodriguez_ToxId_2017,Sridhar_Tracktor_2018}, track animals by segmenting them from the image background and assign identities by stitching traces together across frames. Although the classification accuracy can be quite high under optimal conditions, these methods generally require human intervention to prevent assignment error from propagating over longer timescales even at low error rates (or they are used for analyses where individual identity is not needed). 

Both approaches to identity tracking can be used to study complex social and individual behaviors, but the computational cost of collision resolution means that tracking is generally performed off-line on recorded video data \cite{Liu_A_2018}. Furthermore, the need to record uncompressed or lightly compressed high-resolution video data can make it challenging to track continuously over timescales of days or weeks. Some methods of postural segmentation require manual additional of limb markers \cite{Kain_Leg_2013}, splines fit in post processing \cite{Uhlmann_FlyLimbTracker_2017}, or computationally heavy machine vision in post processing \cite{Mathis_DeepLabCut_2018,Pereira_Fast_2018}. In all cases, the need to separate tracking and recording (in addition to other requirements such as high spatial resolution or human intervention) can be rate-limiting for experiments. Real time tracking offers the benefits of allowing closed-loop stimulus delivery and a small data footprint due to the lack of a need to retain video data. In general, real-time tracking methods are less capable of tracking individuals through collisions because they cannot use future information to help resolve ambiguities \cite{Itskovits_A_2017}. For that reason, real time multiple animal trackers often rely on spatial segregation of animals to distinguish identities or dispense with identity tracking altogether\cite{Liu_A_2018}. Some existing real time trackers can track multiple animals (without maintaining their identity through collisions) in parallel and support a variety of features such as modular arena design, and closed-loop stimulus delivery \cite{Geissmann_Ethoscopes_2017,Straw_Multi_2011,Stowers_Virtual_2017,Chagas_The_2017}.

The tracking algorithms, software interface, hardware configurations, and experimental goals of available trackers vary greatly. Some packages such as Tracktor and FlyWorld use a simple application programming interface (API) and implement tracking through background segmentation and match identities with Hungarian-like Algorithms that minimize frame to frame changes in position. Ethoscopes are an integrated hardware and software solution for high-throughput behavioral experiments that uses real time tracking based on background segmentation. They are unique in their support for modular arenas and peripheral hardware for stimulus delivery \cite{Geissmann_Ethoscopes_2017} and can be networked and operated through a simple web-based interface to conduct experiments remotely and at scale. Ethoscopes provide a hardware template and API for integrating peripheral components into behavioral experiments but are currently not designed to operate outside of the Ethoscope module.  BioTracker offers a graphical user-interface (GUI) that allows the user to select from different tracking algorithms with easily customized tracking parameters or import and use a custom algorithm \cite{Mnck_BioTracker_2018}. 

We wanted to develop a platform that could integrate many of the positive features of these trackers into a single software package and build on them to provide a powerful platform for implementing custom high-throughput experiments with minimal assumptions about the configuration of the experiment. We wanted to emphasize: 1.) fast and accurate individual tracking that could be scaled to very large numbers of individuals or experimental groups over very long time-scales, 2.) flexibility in the user interface that would permit a diversity of organisms, tracking algorithms, experimental paradigms, and behavioral arenas, 3.) integration of peripheral hardware to enable stimulus-driven assays, and  4.) ease of use in the user-interface and data output. 

We developed MARGO, a MATLAB based tracking suite, with these goals in mind. We found that MARGO can reliably track up to thousands of individuals simultaneously in real time over up to days-long timescales. MARGO has two tracking modes that allow it to distinguish individual identity when spatially segregated or population identity when occupying the same arena . We show that traces acquired in MARGO are comparably accurate to those of other trackers and are robust to noisy images and changing imaging conditions. We also demonstrate that tracking requires little to no special hardware and works reliably with minimalist configurations of commonly available equipment (like smart phone cameras). We designed MARGO to require little parameter configuration and provide visual feedback on tracking performance to make it simple and fast to setup new experiments. Additionally, we demonstrate that integration of peripheral hardware into our tracking suite enables closed-loop individual stimulus delivery in high-throughput paradigms. Using adult fruit flies, we demonstrate two closed-loop applications in MARGO for delivering visual stimuli to multiple animals in parallel. The first application is an assay for measuring individual phototactic bias in a Y-shaped arena, and the second is an assay for quantifying individual optomotor response. Though MARGO was developed and tested with fruit flies, we show that it can be used to track many small organisms such as fruit fly larvae, nematodes, and larval zebrafish. We packaged MARGO with an easy-to-use graphical user interface (GUI) and comprehensive user manual to improve the accessibility of the software and offer it as a resource to the ethology community. Though it does not perform visual identity recognition or postural limb tracking, we believe that MARGO can meet the needs of many large behavioral screens, experiments requiring real time stimulus delivery, and users looking to run rapid pilot experiments with little setup.

\section*{Results}

\textit{MARGO Description}

In designing MARGO, we decided that it should: 1.) facilitate identity preservation by allowing many distinct spatial tracking regions to be defined and tracked independently, 2.) be robust to imaging perturbation and have a minimal data footprint to enable long-term recording, 3.) have high throughput compatible with large behavioral screens, 4.) facilitate low latency closed-loop, individual stimulus delivery, and 5.) work for many organisms, behavioral platforms, and imaging configurations.

The core experimental workflow of a MARGO experiment (Fig. 1A) can be briefly summarized as follows: 1.) define spatial regions of interest (ROIs) in which flies will be tracked, 2.) construct a background reference image used to separate foreground and background, 3.) sample imaging statistics of clean tracking to facilitate detection and correction of noisy imaging, 4.) perform tracking.  We found that constraining the space that each animal can be tracked significantly relaxed the computational requirements multi-animal tracking. To fully take advantage of the throughput offered by this approach, we insisted on strategies of ROI definition that would make it simple to rapidly define thousands of ROIs. MARGO initiates tracking setup by defining ROIs through one of two modes. The first is an automated detection mode that detects and segments regular patterns of high-contrast regions in the image, such as back-lit arenas. The second mode prompts the user to manually place grids of ROIs of arbitrary size. In practice, we find that ROI definition typically takes a few seconds but can take as long a few minutes.

Following ROI definition, a background reference image is constructed for each ROI separately. Tracking is performed using a well-known strategy that consists of segmenting binary blobs from a thresholded difference image computed by subtracting each frame from an estimate of the background (Fig. 1B). Background subtraction commonly suffers from two issues with opposing solutions. The first is that subtle changes in the background over time introduce error in the difference image, requiring continuous averaging or reacquisition of the background reference image. The second issue is that continuous averaging or reacquisition of the background can make inactive animals appear as part of the background rather than foreground, making them undetectable in the thresholded difference image. Constructing the reference for each ROI separately mitigates these concerns by allowing the reference to be constructed in a piece-meal fashion when the animals are guaranteed not to become part of the background. Once a background reference image is established, tracking can begin. Candidate blobs in the foreground are filtered by size and sorted to ROIs by spatial location. ROIs with multiple candidates are assigned the best candidates based on minimizing frame to frame changes in position.

Degradation of difference image quality over time (due changes in the background, noisy imaging, and physical perturbation of the imaging setup) constitutes a significant barrier to long term tracking \cite{Sridhar_Tracktor_2018}. The sensitivity of difference images to minor frame to frame changes makes them particularly well suited to low-resolution tracking on complex backgrounds \cite{Liu_A_2018} but also makes the tracking fragile to noise introduced by small changes in the image background. To address this problem, MARGO continuously monitors the quality of the difference image and updates or reacquires the background reference image when imaging becomes noisy. We refer to this collective process as noise correction. Prior to tracking, the software samples the distribution of the total number of above-threshold pixels under clean imaging conditions to serve as a baseline for comparison.  During tracking, the software then continuously calculates that distribution on a rolling basis and reacquires a reference image when the rolling sample significantly deviates from the baseline distribution.

\textit{MARGO is accurate and robust to noise}

We performed a number of experiments and analyses to assess MARGO's robustness to tracking errors and comparability with other trackers. In these experiments, we tracked individual flies, each alone in a circular arena. Thus, we used MARGO in the mode where individual identity is assured by spatial segregation.

We assessed the ability of MARGO to handle degradation of the difference image by repeatedly shifting the background reference image by a small amount (2px, 2\% of the arena diameter, and 0.16\% of the width of the image) to simulate reference image inaccuracy caused by misalignment (mimicking situations where an accidental nudge or vibration shifts the arena). MARGO was used to simultaneously record a movie of individual flies walking in circular arenas and track their centroids. These tracks were the ground truth for this misalignment experiment, and background shifting was implemented digitally on the recorded movie. Misalignment was repeatedly introduced on a fixed interval by shifting the background reference image by 2 pixels in a random direction. Trial triggered averaging of the tracking error shows that MARGO detects changes in difference image statistics and recovers clean tracking by reacquiring the background ,typically within 1 second (Fig. 2D). Forcing reacquisition of the background reference image has the disadvantage of initializing the reference with a single image, meaning that a reference image built by median-filtering multiple frames spaced in time cannot be computed immediately (reference images made this way have two benefits: lower pixel noise and fewer tracking dead spots because they do not include moving animals). We found that this typically causes a reduction in tracking accuracy that is brief (<2s) and had little effect on the overall correlation of the tracking data to the ground truth (r=0.99). Indeed, we found a small effect on tracking error (mean 3.07+- 2.5 pixels) even when shifting the background every 2 seconds. In our experimental set-ups, noise-induced background reacquisition was relatively rare, typically occurring fewer than 10 times over the duration of a two hour experiment.

We tested MARGO's sensitivity to video compression by compressing and tracking a video previously captured during a real time tracking session. The centroid position error of traces acquired from compressed videos were calculated by comparing them to the ground-truth traces acquired on uncompressed images in real time. MARGO showed sub-pixel error in tracking up to 3000-fold compression, notable considering that the mean area of each fly was 15px (Fig. 2A). We further tested the robustness of MARGO to noisy imaging by digitally injecting pixel noise into the thresholded difference image of each frame of a video previously acquired and tracked under clean conditions. Noise was added to the binary threshold image (by randomly setting each pixel to True with a uniform probability) downstream of the background image reacquisition process described above and upstream of tracking to simulate tracking under conditions where noise correction is poorly calibrated. Tracking error was measured as the distance in pixels between tracked positions in each frame of the noisy video and the unaltered source video.  We measured sub-pixel level error up to 20\%/pixel noise (fig. 2B). In practice, we find it easy to create imaging conditions with noise levels <1\%/pixel without the use of sophisticated tracking setups.

To compare the tracking accuracy of MARGO to a widely used animal tracker, we fed uncompressed video captured during a live tracking session in MARGO into Ctrax\cite{Branson_High_2009}, a popular open-source tracking software package, and measured the discrepancy between the two sets of tracks. Overall we found a high degree of agreement between traces acquired in MARGO and Ctrax (Fig. 2E-F). We attribute the majority of discrepancies to minor variations in blob size and shape arising from differences in background models and segmentation from the background. It is worth noting that although Ctrax flagged many frames for manual inspection and resolution, for comparability we opted not to resolve these frames and instead restricted our analysis to the automatically acquired traces. (Ctrax primarily uses these flags to draw user attention to tracking ambiguities through collisions, which did not happen in our experiment because flies were spatially segregated.) Manual inspection of track frames with error larger than 1 pixel revealed that most major discrepancies occurred in one of two ways 1.) short periods between the death and birth of two traces on the same animal in Ctrax 2.) identity swaps in Ctrax between animals in neighboring arenas. These errors may be attributable to our inartful use of Ctrax.

\textit{MARGO is well suited to massive behavioral screens}

We designed MARGO with high-throughput behavioral screens in mind, where hundreds or thousands of animals may need to be tested in each of hundreds of experimental groups. Many features in MARGO's GUI have been included to reduce the time needed to establish successful tracking. These include features to visualize object statistics and the effects of adjusting various tracking parameters. Additionally, we added the ability to save and load parameter and experimental configurations, meaning that users only need to dial in the tracking parameters of a particular experiment once. We found that features such as visual feedback on parameters, parameter profile loading GUI, automated ROI detection, and the lack of model training in MARGO's tracking algorithm all substantially reduce the time needed to set tracking up. 

The speed of the tracking algorithm permits the tracking of very large numbers of animals simultaneously in a single field of view (facilitating certain aspects of experimental design, like testing multiple experimental groups simultaneously). To measure MARGO's speed, we recorded the mean real-time tracking rate while systematically varying the number of tracked ROIs. We found that the frame to frame latency scaled linearly as a function of the number of ROIs tracked [fig. 2]. On modern computer hardware (intel i7 4.0GHz CPU), we measured tracking rates of 160Hz for a single ROI down to 5Hz for 2400 ROIs. We found that frame rates above 5Hz were sufficient to capture the major features of individual movement bouts in adult fruit flies. MARGO could plausibly track up to 5000 animals at lower rates (~1 Hz), potentially fast enough for experiments monitoring changes in activity level changes over long timescales, like circadian experiments. 

Large behavioral screens can potentially generate hundreds of hours of data on thousands of animals. The volume, diversity, and architecture of data can be difficult to work with even without the need to record videos. We found that experiments tracking many hundreds of animals over multiple days can generate raw data files too large to hold in memory on typical modern computers. We designed a custom data container for MARGO and an API to rapidly and dynamically access data from large binary files. MARGO's raw data API includes methods to facilitate batch processing of multiple tracking experiments or single datasets that are too large to hold in computer memory (see user manual). 

\textit{MARGO is simple and flexible}

To demonstrate the simplicity of prototyping experiments in MARGO without the need for specialized hardware or imaging configurations, we ran a tracking experiment under minimal conditions using only commonly available materials.  Individual fruit flies were placed into the wells of a standard 48 well culture plate. The plate was put in a cardboard box to reduce reflections on the well plate and placed on a sheet of white paper as a high contrast background. Movies were recorded on a 1.3MP smartphone camera using natural room light as illumination and imported into MARGO for tracking (supplemental video 1). Tracks and movement bouts acquired under these conditions showed no apparent differences to those acquired under optimized conditions (i.e., custom arenas over diffused LED illuminators in light-sealed imaging boxes). We found that the lower contrast illumination of this minimal setup introduced moderate imaging noise and narrowed the functional range of parameters that worked for foreground segmentation, but had no apparent effect on the accuracy of traces once calibrated.

MARGO was developed for high-throughput ethology in fruit flies, but many small organisms used for high-throughput behavior (e.g., nematodes, larval zebrafish, and fruit fly larvae) are more translucent than adult flies. To assess MARGO’s tracking robustness on such organisms, we used MARGO to track videos of larval \emph{Danio rerio}, \emph{Caenorhabditis elegans}, larval \emph{Drosophila}, and Bumblebees. As expected, the translucency of these organisms narrowed the functional range of some tracking parameters, but we found that the real-time feedback on parameter performance in MARGO's GUI made it simple to dial in the tracking for these new organisms. Sample traces acquired from other organisms were qualitatively similar to those acquired with adult flies, suggesting that MARGO’s high-throughput, ROI-based tracking approach is applicable to a variety of organisms.

We gave MARGO a graphical user interface (GUI) to make its full functionality accessible to users unfamiliar with MATLAB or programming in general [figraf]. We generally find that new users easily learn to use both the work-flow and parameter customization. Furthermore, live visual feedback on the effects of parameters and options make it easy to configure tracking for a variety of experimental paradigms. We found that typical setup of tracking ranged between a few seconds with saved parameter profiles to a few minutes under novel imaging conditions. The utility of the GUI extends beyond tracking setup to include customization of analysis, visualization, and input/output sources such as videos, cameras, displays, and COM devices. Complete details on using the GUI, setting up tracking, utilizing the data output, and defining custom experiments via the API are available in the user's manual (supplemental materials).


\textit{MARGO integrates peripheral hardware for closed-loop stimulus control}

Real time tracking opens up the possibility of delivering closed-loop stimuli that depend on the behavior of animals. MARGO offers native support for the hardware needed for closed-loop experiments including: cameras for real time image acquisition, projectors/displays for visual stimuli, and serial COM devices for digital control of other peripheral electronics. The spread and adoption of easily-configurable consumer microcontrollers (like Arduinos REFREF and Raspberry Pi single board computers REFREF) makes it relatively simple to control a wide variety of devices. For this reason, MARGO was designed to detect and communicate with configurable COM devices devices to integrate real time feedback from external sensors and coordinate closed-loop control of electronic hardware. 

We built and conducted experiments with a custom circuit board to measure individual phototactic preference (the "LED Y-maze"). In this assay, flies separately explored symmetrical Y-shaped arenas with LEDs at end of each arm of the arenas (fig. 4). For each arena in parallel, real time tracking through MARGO detected which arm of the maze was occupied at each frame. At the start of each trial, an LED was randomly turned on in either the right or left  of the occupied arm. Margo turned off all LEDs in the arena When a fly was detected changing position from one arm to another. Immediately following a change of arm, a new trial was initiated by randomly turning on an LED to the right or left of the newly occupied arm. This process repeated for each fly independently over two hours until an experiment ended. Margo scored choices as positively or negatively phototactic when flies turned to the lit and unlit arms respectively. This assay has structural similarities to an assay we previously used to measure locomotor handedness, the tendency of individual to turn left or right when going through the center of the arena. In the LED Y-maze assay, locomotor left-right decisions were made in superposition with light-dark choices. Flies typically make hundreds of choices over the course of an experiment, giving us enough data to examine the turn bias of individuals under all four left-right/light-dark conditions (fig. 4). Tiling many such mazes on a single board yielded the experimental throughput for which MARGO is well-suited. Overall, we recorded choices from over 3,600 individuals, representing more than 830,000 choices in total.

To assess MARGO's capacity to reveal behavioral differences between genotypes, we tested a variety of wild type strains in the LED Y-maze. All wild type strains exhibited a significant average positive phototactic bias (REFREF insert number). In contrast, blind flies (\emph{Norp-A} mutants) and flies under identical circumstances but without providing power to the LEDs, showed mean "preferences" indistinguishable from 0.5, consistent with random choices (fig. 4). The lines tested showed significant variation in mean population preference (REFREF stats) and individual variability (REFREF stats). We were curious about the interaction of phototactic and locomotor biases. We divided trials into two groups based on whether the lit LED appeared to the right or left of the choice point. We found that the mean turn bias but not the mean phototactic bias differed between these two conditions (fig REFREF). Categorizing trials this way revealed that the rank order of both phototactic bias and turn bias are anti-correlated (REFREF#) between the two conditions, suggesting that phototactic and locomotor handedness affect the inform individual choices. 

Fast real time tracking offers the benefit of tight and accurate stimulus modulation in response changes in behavior. We adapted a previously described optomotor paradigm to a high-throughput configuration in MARGO \cite{Fujiwara_A_2017} to test MARGO's ability to deliver a stimulus dependent on accurate and rapid feedback on a frame-to-frame basis. Under this paradigm, an optomotor response is elicited by targeting a high-contrast, rotating pinwheel directly beneath a freely walking fly. On average, optomotor stimuli evoke a turn in the direction of the rotation to stabilize the visual motion \cite{Gtz_Visual_1973}. The center of the pinwheel follows the position of the fly as it moves around the arena so as to keep the apparent visual motion constant on the fly's eye. This paradigm depends on fast accurate feedback because inaccurate centering of the stimulus on the animal results in partial disagreement in the apparent direction of visual motion.

We constructed a behavioral platform with a camera and an overhead mounted projector targeting an array of flat circular arenas (fig. 5A). We added a MARGO feature to track small dots displayed by the projector with the camera. By recording the apparent location of displayed dots in the camera's field of view (FOV), we constructed a registration mapping from the camera FOV to the projector display field. Using this mapping to complete a closed loop between the camera and projector, we used the tracked positions of flies in Margo to target projected pinwheel stimuli independently to multiple freely moving individuals in parallel (fig. 5B). To ensure faithful coordination between the tracking and stimulus delivery, the tracking rate was matched to the refresh rate of the display at 60Hz. Targeting stimuli this way, we observed that strong optomotor responses could only be reliably elicited when individuals were moving prior to the presentation of the stimulus, a finding consistent with previous observations of the dependence of optomotor response on arousal state \cite{Zhu_Peripheral_2009,Kim_Fly_2016}. We thus required the following set of criteria to be met for stimulus presentation: 1) flies were moving 2) a minimum wait period between trials had passed 3) flies were above a minimum distance to the edge of the arena. We included an inter-trial interval to serve as a baseline for comparison to stimulus evoked behavior. Minimum distance to the edge ensured that the stimulus occupied a significant portion of the animal's field of view. 

We independently targeted pinwheel stimuli to individuals over a two hour period in two second bouts with a minimum inter-trial wait period of two seconds using the above constraints (fig. 5C). In total, over 300,000 trials were recorded from more than 1,800 flies. For each trial, we calculated an optomotor index as the fraction (normalized to a range -1 to 1) of body angle change that occurred in the same direction as the stimulus rotation, comparable to previous descriptions of similar metrics \cite{Seelig_Two_2010}. An optomotor index of one indicated all body angle change occurred in the rotational direction of the stimulus. On average, flies displayed strong optomotor responses when presented with high-contrast stimuli (fig. 5D), showing substantial individual variation in both optomotor index and trial number (fig. 5E). We randomly varied stimulus contrast, angular velocity, and spatial frequency in parallel on a trial-by-trial basis to measure the relationship between stimulus parameters and to demonstrate the simplicity of parameter optimization in such a high-throughput configuration. Mean population response generally increased with stimulus contrast but saturated over much of the dynamic range of the projector, plateauing around 25\% contrast (fig. 5 F). Similarly, optomotor index increased with both stimulus spatial frequency and angular speed, peaking at 0.18 cycles/degree and 360 degrees/s respectively (fig. 5 G). The optomotor index reversed at high combined values of spatial frequency and angular speed due to the apparent reversal of the stimulus at frequencies higher than the refresh rate of the projector.


\section*{Discussion}

We developed MARGO to accommodate a wide-variety of behavioral paradigms, tracking implementations, and organisms without sacrificing the throughput necessary conduct experiments on a large scale. MARGO represents a powerful and flexible interface for real time tracking. In particular, we envision MARGO aiding in applications that require 1.) very high experimental throughput such as massive behavioral screens and 2.) high-acquisition rates such as closed-loop stimulus targeting. MARGO's tracking algorithm, interface, and data footprint are light-weight, making it perform particularly well in applications where greater computational costs would be rate limiting. The ability to rapidly define and track thousands of ROIs enables MARGO to track and deliver stimuli to each individual separately. Furthermore, MARGO's simple user-interface and comprehensive user manual make it accessible both to new users trying to prototype or run pilot experiments and more advanced users looking to develop custom experimental paradigms.

We designed MARGO to make it trivial to define hundreds or thousands of ROIs. Tracking flies in spatially distinct ROIs offers the ability to preserve identities indefinitely without the need to resolve collisions manually or with intense algorithms, and can facilitate accurate tracking with very few assumptions about the size and morphology of tracked objects. The use of spatial segregation dramatically improves the throughput of large experiments where accurately maintaining identity is important not only by acquiring video and tracking simultaneously but also by removing the requirement for human supervision and intervention. We found that insisting on spatial segregation ultimately relaxes the computational requirements enough that thousands of individuals can be tracked in real time. An ROI-based architecture can also be used to distinguish groups rather than individual identities by separating groups into distinct arenas. This configuration therefore allows multiple populations, as well as individuals, to be tested in parallel. Some existing trackers allow multiple ROI definition but require each ROI to be drawn manually \cite{Prez-Escudero_idTracker_2014,Mnck_BioTracker_2018}.

Long-term automated measurement of activity in animals to study sleep, circadian rhythms, drug effects, and aging constitutes a major interest of ethology. MARGO offers many features useful for activity measurement over long timescales. Rapid experimental setup, small data footprint, and built-in utilities for handling large data sets make make ideal for extended behavioral measurement. We believe that this framework is easily extensible to numerous experimental applications comparing behavior of individuals, genotypes, or treatment groups.  

Many interesting animal behaviors are elicited by stimuli. To bring the full benefit of experimental automation to the study of animal behavior, it is important that methods for automated stimulus delivery are included. With built-in support for interfacing with cameras, displays, and peripheral electronics, MARGO is designed for stimulus-driven ethology on a massive scale. The ability for precise temporal and spatial targeting of light with projector displays opens up a wide-variety of closed and open loop visual and optogenetic experiments. Native detection and communication with serial COM devices further extends these capabilities by providing a generic interface for a wide variety of peripheral devices. Taken together, we envision MARGO as a multi-purpose platform for coordinating the various activities of hardware inputs and outputs needed to conduct high-throughput ethology. 

Here we demonstrated two such applications, an individual phototaxis assay in an array of Y-shaped arenas and an assay for reliably eliciting optomotor response of many individuals in parallel. In total, we screened nearly 5,000 animals over hundreds of thousands of trials on these two platforms, representing a deep profile of both individual and population level behavior. With the experiments themselves representing less than 2 days of recording, these platforms could easily be extended to large behavioral screens of covering hundreds genetic lines. In the LED Y-maze, we showed that individuals displayed bias in both phototactic preference and locomotor handedness. The wild-type fly lines we screened displayed population level differences in both mean preference and variability in phototactic bias. Furthermore, in at least one of these lines, the population mean shifted dramatically from a negative to positive phototactic bias over the first week post-eclosion. Interestingly, the mean difference and negative rank correlation between turn bias on trials when the light appeared on opposite sides of the starting tunnel suggests phototaxis and handedness appear to be interactive. Although the lower mean difference in phototactic bias between these two conditions suggests that the weight of phototactic preference is stronger than that of handedness, we found evidence that locomotor handedness produces measurable effects in the prevalence of an ethologically salient stimulus such as light. 

In the optomotor platform, we demonstrated that, along with a simple hardware setup, MARGO can be used quantify optomotor response en masse in an open arena. Interestingly, in this context the optomotor response was largely suppressed when delivered to stationary flies, suggesting that the response may be gated by movement. While all animals tested exhibited the optomotor response, we found that individuals showed considerably more variability in their responses than our null model, suggesting that individuals may have idiosyncratic sensitivity to the stimulus. Of note is the fact that we reliably elicited responses with stimuli displayed with a perceptible flicker at 60Hz, well below the 200Hz flicker fusion frequency of fruit flies. Many previous studies on fruit fly optomotor response have expended considerable effort to faithfully display optomotor stimuli at or near the flicker fusion rate. Given the limitations of our platform, we were able to demonstrate that optomotor response can be elicited at lower display rates. We attribute much of the success of this platform to the ability to tightly maintain the position of the stimulus centered on the animal. One attractive extension of this accurate spatial and temporal control of light is closed-loop optogenetic manipulation of many animals in parallel conditioned on particular behaviors.

Behavioral experiments are frequently more complex than tracking objects in a dish. Custom experiments routinely require complex arena geometries, data streams from external sensors, control of peripheral hardware, and time-series data of various object features. MARGO's ability to manage these experimental components make it well-suited to defining new stimulus-driven behavioral paradigms. MARGO was designed to run experimental modules that define how to coordinate the various activities of custom experiments. The ROI definition strategies employed here are robust to many arena geometries with minimal constraint on the positioning or regularity of ROIs within the field of view. Templates for new experiments with custom inputs and outputs can be automatically generated and detected from within the GUI. In practice, we find that new experiments with customized output and hardware interfaces can typically be defined in 1-2 simple functions with some knowledge of the API.

Animal tracking platforms are evolving to meet the diverse needs of the ethology community. While MARGO has been designed to integrate many features of existing platforms under a single framework, we recognize that many users will want to run experiments that are more appropriately implemented on existing platforms. We have included a comparison of features (Table 1) in several tracking programs that might impact which program is best suited to particular applications. The ability to resolve identities through collisions in packages such as idTracker, FlyTracker, and CTrax offers a powerful solution to study the behaviors of individuals in social networks. Though many trackers, including MARGO, can track multiple animals occupying the same spatial region, the ability to distinguish identities across time is essential in probing social dynamics. Ethoscopes offer a unique hardware framework and rich API for conducting diverse behavioral experiments, including stimulus driven ethology, in a distributed fashion. Furthermore, several Ethoscope behavioral modules are adapted to fit single fly vials with food, which enables tracking of long timescales. The BioTracker platform is a great interface for importing and testing custom tracking algorithms and offers the additional benefit running without a third-party environment due to its implementation in C/C++.  We envision MARGO as an additional resource to the community with the potential to run a broad spectrum of behavioral experiments but believe that, in particular, it is well-suited to experiments that benefit from fast real time tracking or closed-loop control of peripheral hardware. 

\section*{Methods}

\noindent\textit{Software}
\vspace*{0.3cm}

The MARGO GUI, tracking algorithm, and all analysis software were written in MathWorks MATLAB. Detailed descriptions of the function and use of the MARGO GUI, ROI detection, background referencing, tracking implementation, noise correction, and data output are available in MARGO user manual. Optomotor stimuli were crafted and displayed using the Psychtoolbox-3 for MATLAB. Software for control of all custom electronic hardware was written in C.

\vspace*{0.5cm}
\noindent\textit{Organism genotypes and care}
\vspace*{0.3cm}

Unless otherwise specified, the genotype of all fruit flies tested was an inbred strain of  Berlin K. Tracking experiments were performed with mixed groups of male and female flies 3-5 days post-eclosion unless otherwise noted.  Flies were raised on standard cornmeal media (BuzzGro from Scientiis) under 12 h/12 h light and dark cycle in an incubator at 25°C and 70\% humidity. For behavioral experiments, all animals were lightly anesthetized on ice or CO$_2$ before loading into arenas and were given a minimum of 30 minutes to recover. Animals were imaged and singly-housed on food in individual fly storage (FlySorter LLC) for all multi-day tracking experiments. Sample tracking experiments with other model organisms were all limited to a single genotype. C. Elegans were \textit{N2} and tracked on standard agar media. Drosophila larvae. Larval zebrafish \textit{HuC:GCaMP6s} and were tracked in fish medium containing brine shrimp.

\vspace*{0.5cm}
\noindent\textit{Behavioral Acquisition}
\vspace*{0.3cm}

All live images were captured with overhead mounted USB or GigE cameras from Point Grey (Firefly MV 13SC2 and BFLY-PGE-12A2M-CS). Cameras used in experiments conducted in behavioral modules were fitted with a long-pass 87 Kodak Wratten infrared filter and illuminated with infrared LEDs to insulate tracking images from incident light from visual stimulus delivery. Acquisition rates varied by experimental requirements, 5.0 fps for LED Y-maze and 60.0 fps for optomotor response. Flies were imaged at spatial resolutions ranging between 1-4 pixels per mm and generally have found tracking to be stable at 15 pixels per animal and above.  Off-line video tracking was performed on 1000x compressed AVI video files unless otherwise specified. Tracking and imaging was conducted in Windows 10 on computers with CPUs ranging from intel i3 3.1GHz to intel i7 4.0GHz. 

\vspace*{0.5cm}
\noindent\textit{Behavioral modules}
\vspace*{0.3cm}

Unless otherwise specified, all tracking was conducted in custom imaging boxes constructed with laser-cut acrylic and aluminum rails. Illumination was provided by dual-channel white and infrared LED array panels mounted at the base. Sanded clear acrylic diffusers were placed in between the illuminator and the behavioral arena for smooth backlighting. Vector schematics of custom behavioral arenas were designed in AutoCAD and are available at <refraf insert git link>. Arena parts were laser-cut from black and clear acrylic and joined with Plastruct plastic weld. Additional tracking was performed in standard 48 multi-well culture plates and individual fly storage units (FS-STORAGE-BUNDLE) from FlySorter LLC. Additional details on the behavioral platforms used here are available in the MARGO user manual.

\vspace*{0.5cm}
\noindent\textit{Experimental procedures}
\vspace*{0.3cm}

Tracking experiments were conducted between 10AM and 6PM. Flies were anesthetized either on ice or CO$_{2}$ and manually loaded into behavioral arenas with an aspirator. Behavioral modules were loaded into tracking boxes and allowed a minimum post-anesthesia recovery period of 20 minutes before tracking. Unless otherwise specified, animals were tracked for 2 hours in a micro-environment room at 23\degree  C and 40\% humidity. Following tracking, flies were returned to individual storage plates where they were allowed to feed and recover.

\vspace*{0.5cm}
\noindent\textit{Data and Statistics}
\vspace*{0.3cm}

The raw data and analysis scripts can be found at <refraf deposit the data>. Data pre-processing and calculation of behavioral metrics was conducted automatically by MARGO either in parallel with or immediately following acquisition. Identical individual null models were constructed by bootstrap re-sampling the shared distribution of all individuals 1000 times. Bootstrapped distributions of behavioral metrics were calculated on simulated individual trial sequences made by re-sampling random trials with replacement up to a total trial number drawn from the shared distribution. All reported confidence intervals were calculated as the 95\% confidence interval on the parameter estimate of the population mean.

<\textit{comparison of distributions}?>
<\textit{correlation coefficients}?>
<\textit{p-values/multiple comparisons}?>

\section*{Acknowledgements}

We thank Ed Soucy and Joel Greenwood for help troubleshooting and fabricating the LED Y-maze board; Simon Forsberg, Jamilla Akhund-Zade, and Carolyn Elya for providing crucial beta testing and feedback during MARGO development; Christian Rohrsen for his substantial role in prototyping the projector behavioral rig and camera registration system; Jess Kanwal, Vladislav Susoy, and Robert Johnson for donating larvae, worms, and fish to test MARGO tracking in other species.

\bibliography{margo_references} 
\end{multicols}

\newpage
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{../figures/autotracker_overview.pdf}
	\end{center}
	\caption*{\footnotesize \textbf{Figure 1. MARGO workflow, tracking algorithm, and sample behavioral box} -- A) Diagram of the user workflow to setup a new tracking experiment. Arrow color indicates whether the setup step is required. Before tracking, users define an input source, define ROIs to track, initialize a background reference image used to separate foreground and background, and sample the image statistics on a reference of clean tracking. Multiple points (blue arrows) are provided for customization of tracking parameters. B) Flowchart depicting the MARGO's frame-to-frame tracking routine. Each frame consists of image processing (green) to segment foreground from the background, noise estimation (magenta) to assess the quality of foreground segmentation and determine if the current frame can be tracked, and tracking (cyan) of foreground binary blobs. MARGO's tracking algorithm skips noisy frames and re-acquires the background reference image if many consecutive frames are deemed too noisy to track. C) Schematic of the behavioral boxes used to conduct tracking. Behavioral arenas are backlit with an LED illuminator and imaged with an overhead camera. The tracking camera is fitted with an infrared filter to allow light visible to the animals to be controled independently of the tracking illumination. A diffuser panel between the LED backlight and the behavioral arenas is used to achieve even illumination. The camera and LED backlight are both connected to computer for real time tracking and control via MARGO. Such sophisticated imaging boxes are not required but help ensure consistent imaging quality.}
\end{figure}

\newpage
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{../figures/autotracker_performance.pdf}
	\end{center}
	\caption*{\footnotesize \textbf{Figure 2. MARGO tracking accuracy and robustness to imaging noise} -- A) Median [refraf replace with mean] error of tracking performed on the same video at different levels of compression. Sample images of the same fly and frame are shown below. B) Median [refraf replace with mean] error of tracking performed on the same video with different levels of added noise. Pixel noise was manually added to the binary threshold image downstream of image processing and noise correction by converting any given pixel to true at a fixed probability (image noise). Sample images of the same fly and frame, as well as estimated position (red circle) are shown below. C) Diagram of the background reference image shifting scheme used to simulate background inaccuracy. D) Trial-triggered average tracking error centered on reference shifting. E-F) Sample trace comparison and log distribution of tracking error between traces acquired from the same video in both MARGO and Ctrax. The 95\% confidence interval of the above means were displayed but are within the thickness of lines.}
\end{figure}

\newpage
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{../figures/LED_ymaze_panel.pdf}
	\end{center}
	\caption*{\footnotesize \textbf{Figure 3. High-throughput phototactic assay in Y-shaped arenas} -- A) Sample schematic of the LED Y-maze and behavioral box. B) Diagram of a single phototaxis maze and trial structure. New trials initiate by turning on (yellow) an LED in one of the two unoccupied maze arms. The trial ends when then animal turns into a new arm and the lit LED is turned off (gray). Each turn is scored for both handedness (left or right of the starting arm) and phototactic preference (positive or negative). C) Raw turn data for two sample flies. Each individual trial consists of both a phototactic and handedness choice. Individual means are typically averaged from hundreds of trials over a two hour period. D) Distribution of individual average phototactic biases for the same cohort of flies over the first 8 days post-eclosion. Horizontal dashed line indicates random bias at p=0.5. E) Comparison of individual average phototactic bias distributions for different wild-type fly lines. Blind flies (NorpA) and flies tested with all LEDs turned off (DGRP-105 dark) are included as negative controls.}
\end{figure}

\newpage
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{../figures/Optomotor_panel.pdf}
	\end{center}
	\caption*{\footnotesize \textbf{Figure 4. High-throughput optomotor assay implementation in MARGO} -- A) Schematic of the optomotor arenas and behavioral box. B) Diagram of a single arena and optomotor stimulus. Trials begin with a pinwheel stimulus projected onto the center of mass of the fly. For each trial, the rotational direction (red arrow) of the stimulus is randomly chosen. The animal's tracked movement is used to maintain the stimulus center on estimated center of mass of the animal as it moves. Trials end when the stimulus is removed after a fixed duration (typically 2s). C) Sample raw individual time series data. Flies typically respond to optomotor stimuli by turning in the direction of the rotation of the stimulus. Hundreds of individual trials are recorded on average over a two hour period. D) Trial-triggered average optomotor response across all individuals. Change in body angle (magenta) is scored as positive or negative relative to the rotational direction of the stimulus. All trials were aligned to have change in body angle of zero at the onset of the stimulus. Average distance to the arena center (cyan) drops immediately preceeding stimulus onset due to the trial structure constraint that flies must be off the arena edge. E) Comparison of the observed distribution of individual average optomotor indices (n=1,860) to an identical individuals null model. The null model distributions were generated by bootstrap resampling trials from the shared distribution of all trials up to a maximum number randomly sampled from observed individual trial numbers. F) Population average optomor index as a function of normalized stimulus contrast. Pinwheel contrast was randomly varied on a trial-by-trial basis. G) Kernel density estimates of population average optomotor index as a function of stimulus spatial frequency and stimulus angular velocity. The 95\% confidence intervals of all above average traces are within the thickness of the line.}
\end{figure}

\newpage
\begin{figure}[t!]
	\begin{center}
		\vspace*{-8cm}
		\includegraphics[width=0.9\textwidth]{../figures/platform_comparison_table.pdf}
	\end{center}
	\caption*{\footnotesize \textbf{Figure 5. Comparison of open-source animal tracking packages} -- Existing software packages offer a variety of features useful to ethologists. We see trackers as falling into one of two broad categories: 1) real time trackers capable of very high-throughput with the potential hardware integration and stimulus delivery 2) off-line trackers capable of tracking postural features and maintaining individual identities without spatial segregation. We imagine MARGO, as well as other platforms such as Ethoscopes and BioTracker, falling into this first category. Experimental modules are a natural extension of real time trackers since tracking and other specialized components of experiments needs can be conducted in parallel. Popular examples of programs in the second category include Ctrax, idTracker, and flyTracker. The comparably more computationally expensive tracking algorithms used as well as the training and calibration process make them unsuitable for real time applications but offer the notable benefits of being able to study fine-scale postural and social behaviors.}
\end{figure}

\end{document}